{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "1:\n  Gradient Boosting Regression is a machine learning algorithm used for regression problems. It works\nby building multiple decision trees iteratively, with each new tree attempting to correct the errors \nmade by the previous ones. At each iteration, the algorithm calculates the gradient of the loss function \nwith respect to the current model's predictions and trains a new tree to predict the residual errors. \nThe final predictions are obtained by combining the predictions of all the trees. Gradient Boosting\nRegression is known for producing highly accurate and robust models, especially when dealing with large\nand complex datasets  ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "2:\n  ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nclass GradientBoostingRegressor:\n    \n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n    \n    def fit(self, X, y):\n        #initialize the prediction to the mean of target variable\n        self.prediction = np.mean(y)*np.ones(X.shape[0])\n        \n        for i in range(self.n_estimators):\n            #Calculate the negative gradient\n            residual = y - self.prediction\n            \n            #Fit a decision tree to the negative gradient\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residual)\n            \n            #Make a prediction using the newly fitted decision tree\n            prediction_i = tree.predict(X)\n            \n            #Update the prediction for the next iteration by adding a fraction of the new prediction\n            self.prediction += self.learning_rate * prediction_i\n            \n            #Save the tree for later use\n            self.trees.append(tree)\n            \n    def predict(self, X):\n        #Make a prediction by summing the predictions of all decision trees\n        prediction = np.zeros(X.shape[0])\n        for tree in self.trees:\n            prediction += self.learning_rate * tree.predict(X)\n        return prediction\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n#generate a random dataset\nnp.random.seed(0)\nn_samples = 100\nX = np.sort(np.random.rand(n_samples))*5\ny = np.sin(X) + np.random.randn(n_samples) * 0.1\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\n#split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#train the gradient boosting model\ngb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\ngb.fit(X_train.reshape(-1,1), y_train)\n\n#make predictions on the testing data\ny_pred = gb.predict(X_test.reshape(-1,1))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_squared_error, r2_score\n\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\n\nprint('R-squared score: %.2f'\n      % r2_score(y_test, y_pred))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": "Mean squared error: 0.06\nR-squared score: 0.88\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "3:\n    ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nclass GradientBoostingRegressor:\n    \n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n    \n    def fit(self, X, y):\n        #initialize the prediction to the mean of target variable\n        self.prediction = np.mean(y)*np.ones(X.shape[0])\n        \n        for i in range(self.n_estimators):\n            #Calculate the negative gradient\n            residual = y - self.prediction\n            \n            #Fit a decision tree to the negative gradient\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residual)\n            \n            #Make a prediction using the newly fitted decision tree\n            prediction_i = tree.predict(X)\n            \n            #Update the prediction for the next iteration by adding a fraction of the new prediction\n            self.prediction += self.learning_rate * prediction_i\n            \n            #Save the tree for later use\n            self.trees.append(tree)\n            \n    def predict(self, X):\n        #Make a prediction by summing the predictions of all decision trees\n        prediction = np.zeros(X.shape[0])\n        for tree in self.trees:\n            prediction += self.learning_rate * tree.predict(X)\n        return prediction\n\n#generate a random dataset\nnp.random.seed(0)\nn_samples = 100\nX = np.sort(np.random.rand(n_samples))*5\ny = np.sin(X) + np.random.randn(n_samples) * 0.1\n\n#split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n#set up the parameter grid for the grid search\nparam_grid = {'n_estimators': [50, 100, 200],\n              'learning_rate': [0.01, 0.1, 1],\n              'max_depth': [3, 5, 7]}\n\n#perform the grid search\ngb = GradientBoostingRegressor()\ngrid_search = GridSearchCV(gb, param_grid, cv=5)\ngrid_search.fit(X_train.reshape(-1,1), y_train)\n\n#display the best parameters and the best score\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\n#make predictions on the testing data using the best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test.reshape(-1,1))\n\n#evaluate the performance of the best model\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\nprint(\"R-squared score: %.2f\" % r2_score(y_test, y_pred))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "ename": "<class 'TypeError'>",
          "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <__main__.GradientBoostingRegressor object at 0x4b14fd8> does not.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m gb \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor()\n\u001b[1;32m     58\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(gb, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#display the best parameters and the best score\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/sklearn/model_selection/_search.py:776\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    774\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 776\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m _check_multimetric_scoring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:503\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    504\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method. The estimator \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator\n\u001b[1;32m    506\u001b[0m         )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scoring, Iterable):\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor evaluating multiple scores, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.model_selection.cross_validate instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m was passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scoring)\n\u001b[1;32m    512\u001b[0m     )\n",
            "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <__main__.GradientBoostingRegressor object at 0x4b14fd8> does not."
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error, r2_score\n\nclass GradientBoostingRegressor:\n    \n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n    \n    def fit(self, X, y):\n        #initialize the prediction to the mean of target variable\n        self.prediction = np.mean(y)*np.ones(X.shape[0])\n        \n        for i in range(self.n_estimators):\n            #Calculate the negative gradient\n            residual = y - self.prediction\n            \n            #Fit a decision tree to the negative gradient\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residual)\n            \n            #Make a prediction using the newly fitted decision tree\n            prediction_i = tree.predict(X)\n            \n            #Update the prediction for the next iteration by adding a fraction of the new prediction\n            self.prediction += self.learning_rate * prediction_i\n            \n            #Save the tree for later use\n            self.trees.append(tree)\n            \n    def predict(self, X):\n        #Make a prediction by summing the predictions of all decision trees\n        prediction = np.zeros(X.shape[0])\n        for tree in self.trees:\n            prediction += self.learning_rate * tree.predict(X)\n        return prediction\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        mse = mean_squared_error(y, y_pred)\n        r2 = r2_score(y, y_pred)\n        return r2\n    \n#generate a random dataset\nnp.random.seed(0)\nn_samples = 100\nX = np.sort(np.random.rand(n_samples))*5\ny = np.sin(X) + np.random.randn(n_samples) * 0.1\n\n#split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n#set up the parameter grid for the grid search\nparam_grid = {'n_estimators': [50, 100, 200],\n              'learning_rate': [0.01, 0.1, 1],\n              'max_depth': [3, 5, 7]}\n\n#perform the grid search\ngb = GradientBoostingRegressor()\ngrid_search = GridSearchCV(gb, param_grid, cv=5, scoring=make_scorer(GradientBoostingRegressor.score))\ngrid_search.fit(X_train.reshape(-1,1), y_train)\n\n#display the best parameters and the best score\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\n#make predictions on the testing data using the best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test.reshape(-1,1))\n\n#evaluate the performance of the best model\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\nprint(\"R-squared score: %.2f\" % r2_score(y_test, y_pred))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "ename": "<class 'TypeError'>",
          "evalue": "Cannot clone object '<__main__.GradientBoostingRegressor object at 0x4f027a8>' (type <class '__main__.GradientBoostingRegressor'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m gb \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor()\n\u001b[1;32m     65\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(gb, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mmake_scorer(GradientBoostingRegressor\u001b[38;5;241m.\u001b[39mscore))\n\u001b[0;32m---> 66\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#display the best parameters and the best score\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/sklearn/model_selection/_search.py:788\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    785\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    786\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[0;32m--> 788\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[1;32m    792\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    793\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[1;32m    794\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    800\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    801\u001b[0m )\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/sklearn/base.py:79\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     86\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m     87\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<__main__.GradientBoostingRegressor object at 0x4f027a8>' (type <class '__main__.GradientBoostingRegressor'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "4:\n In Gradient Boosting, a weak learner is a simple and relatively weak model that can only make slightly\nbetter predictions than random guessing. Examples of weak learners include decision stumps, which are \ndecision trees with only one split, or linear models with a small number of parameters.\nThe idea behind Gradient Boosting is to combine multiple weak learners to create a strong and accurate \nmodel. At each iteration, the algorithm fits a new weak learner to the residual errors of the previous\nmodel, and then adds the two models together. By repeating this process many times, the algorithm can\ngradually improve the overall accuracy of the model.\nThe strength of Gradient Boosting lies in its ability to combine many weak learners into a powerful \nensemble model that can capture complex relationships in the data.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "5:\n    The intuition behind Gradient Boosting algorithm is to iteratively improve the predictions of \na model by adding new weak learners, which learn from the errors made by the previous models. \nThe idea is to build an ensemble of models, where each model is trained to improve the predictions\nof the previous model. In this way, the algorithm is able to learn complex relationships between the\ninput variables and the target variable, and produce highly accurate predictions. ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "6:\n   Gradient Boosting algorithm builds an ensemble of weak learners by training each learner to predict\nthe residual errors of the previous model. In other words, the algorithm starts with a simple model, \nsuch as a decision tree, and then trains a new model to predict the difference between the target variable\nand the predictions of the previous model. This process is repeated iteratively until the desired level of\naccuracy is achieved.  ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "7:\n   The steps involved in constructing the mathematical intuition of Gradient Boosting algorithm are as follows:\n1. Initialize the model with a constant value, such as the mean of the target variable.\n2. Calculate the gradient of the loss function with respect to the current model's predictions.\n3. Train a new model to predict the gradient, or error, of the previous model's predictions.\n4. Combine the predictions of the previous model and the new model to obtain the updated predictions.\n5. Repeat steps 2-4 until the desired level of accuracy is achieved.\n\nThe idea behind this process is to iteratively improve the model by learning from the errors made by\nthe previous models. The final model is an ensemble of weak learners, which work together to produce\nhighly accurate predictions.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}