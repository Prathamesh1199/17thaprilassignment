{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1:\n",
    "  Gradient Boosting Regression is a machine learning algorithm used for regression problems. It works\n",
    "by building multiple decision trees iteratively, with each new tree attempting to correct the errors \n",
    "made by the previous ones. At each iteration, the algorithm calculates the gradient of the loss function \n",
    "with respect to the current model's predictions and trains a new tree to predict the residual errors. \n",
    "The final predictions are obtained by combining the predictions of all the trees. Gradient Boosting\n",
    "Regression is known for producing highly accurate and robust models, especially when dealing with large\n",
    "and complex datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #initialize the prediction to the mean of target variable\n",
    "        self.prediction = np.mean(y)*np.ones(X.shape[0])\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            #Calculate the negative gradient\n",
    "            residual = y - self.prediction\n",
    "            \n",
    "            #Fit a decision tree to the negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            \n",
    "            #Make a prediction using the newly fitted decision tree\n",
    "            prediction_i = tree.predict(X)\n",
    "            \n",
    "            #Update the prediction for the next iteration by adding a fraction of the new prediction\n",
    "            self.prediction += self.learning_rate * prediction_i\n",
    "            \n",
    "            #Save the tree for later use\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        #Make a prediction by summing the predictions of all decision trees\n",
    "        prediction = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            prediction += self.learning_rate * tree.predict(X)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#generate a random dataset\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "X = np.sort(np.random.rand(n_samples))*5\n",
    "y = np.sin(X) + np.random.randn(n_samples) * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train.reshape(-1,1), y_train)\n",
    "\n",
    "#make predictions on the testing data\n",
    "y_pred = gb.predict(X_test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.06\n",
      "R-squared score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('R-squared score: %.2f'\n",
    "      % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# generate a random dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "train_X, test_X = X[:80], X[80:]\n",
    "train_y, test_y = y[:80], y[80:]\n",
    "\n",
    "# define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# perform grid search with cross-validation\n",
    "model = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "mse = mean_squared_error(test_y, best_model.predict(test_X))\n",
    "r2 = r2_score(test_y, best_model.predict(test_X))\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Test MSE: {mse:.4f}, R^2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4:\n",
    " In Gradient Boosting, a weak learner is a simple and relatively weak model that can only make slightly\n",
    "better predictions than random guessing. Examples of weak learners include decision stumps, which are \n",
    "decision trees with only one split, or linear models with a small number of parameters.\n",
    "The idea behind Gradient Boosting is to combine multiple weak learners to create a strong and accurate \n",
    "model. At each iteration, the algorithm fits a new weak learner to the residual errors of the previous\n",
    "model, and then adds the two models together. By repeating this process many times, the algorithm can\n",
    "gradually improve the overall accuracy of the model.\n",
    "The strength of Gradient Boosting lies in its ability to combine many weak learners into a powerful \n",
    "ensemble model that can capture complex relationships in the data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5:\n",
    "    The intuition behind Gradient Boosting algorithm is to iteratively improve the predictions of \n",
    "a model by adding new weak learners, which learn from the errors made by the previous models. \n",
    "The idea is to build an ensemble of models, where each model is trained to improve the predictions\n",
    "of the previous model. In this way, the algorithm is able to learn complex relationships between the\n",
    "input variables and the target variable, and produce highly accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6:\n",
    "   Gradient Boosting algorithm builds an ensemble of weak learners by training each learner to predict\n",
    "the residual errors of the previous model. In other words, the algorithm starts with a simple model, \n",
    "such as a decision tree, and then trains a new model to predict the difference between the target variable\n",
    "and the predictions of the previous model. This process is repeated iteratively until the desired level of\n",
    "accuracy is achieved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7:\n",
    "   The steps involved in constructing the mathematical intuition of Gradient Boosting algorithm are as follows:\n",
    "1. Initialize the model with a constant value, such as the mean of the target variable.\n",
    "2. Calculate the gradient of the loss function with respect to the current model's predictions.\n",
    "3. Train a new model to predict the gradient, or error, of the previous model's predictions.\n",
    "4. Combine the predictions of the previous model and the new model to obtain the updated predictions.\n",
    "5. Repeat steps 2-4 until the desired level of accuracy is achieved.\n",
    "\n",
    "The idea behind this process is to iteratively improve the model by learning from the errors made by\n",
    "the previous models. The final model is an ensemble of weak learners, which work together to produce\n",
    "highly accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
